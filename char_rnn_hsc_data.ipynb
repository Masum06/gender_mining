{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn_hsc_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Masum06/gender_mining/blob/master/char_rnn_hsc_data.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "T2ihWdwlp7Nx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Down From Mega"
      ]
    },
    {
      "metadata": {
        "id": "8nqiFtVXoDRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "3313db21-099e-4d85-b8d9-8323ba85ed23"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/jeroenmeulenaar/python3-mega.git python3mega\n",
        "cd python3mega\n",
        "pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.18.4)\n",
            "Collecting URLObject>=2.1.1 (from -r requirements.txt (line 2))\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/b8/1d0a916f4b34c4618846e6da0e4eeaa8fcb4a2f39e006434fe38acb74b34/URLObject-2.4.3.tar.gz\n",
            "Collecting pycrypto>=2.6 (from -r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/60/db/645aa9af249f059cc3a368b118de33889219e0362141e75d4eaf6f80f163/pycrypto-2.6.1.tar.gz (446kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->-r requirements.txt (line 1)) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->-r requirements.txt (line 1)) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->-r requirements.txt (line 1)) (2.6)\n",
            "Building wheels for collected packages: URLObject, pycrypto\n",
            "  Running setup.py bdist_wheel for URLObject: started\n",
            "  Running setup.py bdist_wheel for URLObject: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/fd/7e/18/ccb55ecc2834f945b769c1ff1df12ca5a14400ccfc58e3c515\n",
            "  Running setup.py bdist_wheel for pycrypto: started\n",
            "  Running setup.py bdist_wheel for pycrypto: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/27/02/5e/77a69d0c16bb63c6ed32f5386f33a2809c94bd5414a2f6c196\n",
            "Successfully built URLObject pycrypto\n",
            "Installing collected packages: URLObject, pycrypto\n",
            "Successfully installed URLObject-2.4.3 pycrypto-2.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'python3mega'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9zvLQHBXoS-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('python3mega')\n",
        "from mega import Mega\n",
        "m = Mega.from_ephemeral()\n",
        "os.chdir('..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D50oD1yhoVWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.download_from_url('https://mega.nz/#!RQgxjTDR!vrQimln7Yvtxgn6lqSmKWW1CAWFmSCM2JbWsQcT9ptA') # HSC Name dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4IDci8fdaYDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4846fbd-ee06-42f1-d537-e383bc01b2ea"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iBty_3rIfNQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Character RNN"
      ]
    },
    {
      "metadata": {
        "id": "WG-S9co5IEQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "c2999003-02b6-4ab3-e38f-6b90c097f974"
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.19.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BrSTmoIKHuGD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import itertools\n",
        "import keras\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlqkCFi_D7_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSzzIa81G3Bt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional\n",
        "from keras.layers import LSTM, GRU\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fileName = './hsc_name_gender.csv'\n",
        "\n",
        "# Lowering the texts and replacing with numeral value\n",
        "df = pd.read_csv(fileName, encoding='utf-8')\n",
        "df.columns = ['name', 'gender']\n",
        "df['name'] = df['name'].str.lower()\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Getting all available strings to find out the characters used\n",
        "all_string = \" \".join([name for name in df['name']])\n",
        "unique_characters = list(set(list(all_string)))\n",
        "\n",
        "# Mapping between character to index and index to character\n",
        "char2idx = { c : i for i, c in enumerate(unique_characters) }\n",
        "idx2char = {c: k for k, c in char2idx.items()}\n",
        "\n",
        "# Converts a name into vector\n",
        "def name2vector(name):\n",
        "    chars = list(name)\n",
        "    vector = [ char2idx[c] for c in chars ]\n",
        "    return np.array(vector)\n",
        "\n",
        "# Converts names to fixed size tensor\n",
        "def names2tensor(names, maxlen=25):\n",
        "    namelist = [name2vector(name) for name in names]\n",
        "    return sequence.pad_sequences(np.array(namelist), maxlen=maxlen)  # root of all troubles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8dCZDwCwD7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "0a5ac66f-5338-4ae5-d7d2-2ae2ead6a20a"
      },
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "X = np.array(names2tensor(df['name'], maxlen=25)) #\n",
        "Y = np.array([int(g) for g in df['gender']])\n",
        "\n",
        "X, Y = shuffle(X, Y, random_state=42)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) #changed\n",
        "\n",
        "# max features\n",
        "max_features = 20000\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "\n",
        "# Model building and training\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5)) #changed\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']             \n",
        "             )\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 2121126 samples, validate on 909055 samples\n",
            "Epoch 1/1\n",
            "  17824/2121126 [..............................] - ETA: 1:32:07 - loss: 0.4723 - acc: 0.7631"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2121120/2121126 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2121126/2121126 [==============================] - 5911s 3ms/step - loss: 0.1172 - acc: 0.9570 - val_loss: 0.0765 - val_acc: 0.9745\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97449, saving model to weights-improvement-01-0.97.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5ed9944e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "gipAv4ofyudg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "960fa785-bce4-4222-e834-dae48bd0f299"
      },
      "cell_type": "code",
      "source": [
        "print('Train...')\n",
        "model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs, callbacks=callbacks_list, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Train on 2121126 samples, validate on 909055 samples\n",
            "Epoch 1/1\n",
            "  19488/2121126 [..............................] - ETA: 1:29:11 - loss: 0.0944 - acc: 0.9682"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 287424/2121126 [===>..........................] - ETA: 1:18:14 - loss: 0.0916 - acc: 0.9681"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8clmgRtMA5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 4\n",
        "from keras.models import load_model\n",
        "\n",
        "model.save('char_rnn_hsc_model_{}.h5'.format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nmoIBAepzvoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQ96qaIRrOVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "095d0d9d-3305-426b-e428-dc205f05bfd1"
      },
      "cell_type": "code",
      "source": [
        "!curl --upload-file ./char_rnn_hsc_model_2.h5 https://transfer.sh/char_rnn_hsc_model_2.h5"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://transfer.sh/xvS43/char_rnn_hsc_model_2.h5"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LmEyTQ0Zy1xQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4ad9fb3f-e5e1-42cb-a11a-82caf703bf03"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_rnn_hsc_model_1.h5  \u001b[0m\u001b[01;34mdatalab\u001b[0m/             \u001b[01;34mpython3mega\u001b[0m/\r\n",
            "char_rnn_hsc_model_2.h5  \u001b[01;34mdrive\u001b[0m/               weights-improvement-01-0.97.hdf5\r\n",
            "char_rnn_hsc_model_3.h5  hsc_name_gender.csv  weights-improvement-01-0.98.hdf5\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PuMBGkBuzDnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   https://transfer.sh/AqEf7/char_rnn_hsc_model_1.h5: 97.45\n",
        "*   https://transfer.sh/xvS43/char_rnn_hsc_model_2.h5: 97.55\n",
        "*   char_rnn_hsc_model_3.h5: 97.61\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "v2071cph1gXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Connect to drive, save the model"
      ]
    },
    {
      "metadata": {
        "id": "DxqotBND1kfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9ebfebec-b4cd-4d1b-88b5-bfe735d6068e"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WGPaQfoj1uIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p7bqnlKJ1xcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp char_rnn_hsc_model_4.h5 drive/deeper/data/char_rnn_hsc_models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_0YuTeqz7LW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Load model and run"
      ]
    },
    {
      "metadata": {
        "id": "uy-pu5jj1k41",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp drive/deeper/data/char_rnn_hsc_models/char_rnn_hsc_model_3.h5 ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eR9BEGqP0B2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load('char_rnn_hsc_model_3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIjjjiEys3hm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Dataframe"
      ]
    },
    {
      "metadata": {
        "id": "pMEKoeF-s5-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1181
        },
        "outputId": "17005b5e-1835-4112-81c5-927a1b04c689"
      },
      "cell_type": "code",
      "source": [
        "df.head"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                       en_name  gender\n",
              "0                  saju ahmed       1\n",
              "1              md martuj  ali       1\n",
              "2                rushna begum       0\n",
              "3                nazira akter       0\n",
              "4                    ali miah       1\n",
              "5                 saira begum       0\n",
              "6              khaled hossain       1\n",
              "7              md jamal uddin       1\n",
              "8                 rajna begum       0\n",
              "9                shanta akter       0\n",
              "10                ahsan uddin       1\n",
              "11               latifa begum       0\n",
              "12              sokhina begum       0\n",
              "13                tajul islam       1\n",
              "14               forida begum       0\n",
              "15               ariful islam       1\n",
              "16               azizul haque       1\n",
              "17               fatema begum       0\n",
              "18           md jakirul islam       1\n",
              "19              md abu siddik       1\n",
              "20               majeda begum       0\n",
              "21               shuva  begum       0\n",
              "22                badsha miah       1\n",
              "23               hakima begum       0\n",
              "24               azizul hakim       1\n",
              "25                  anjob ali       1\n",
              "26             shahina khanom       0\n",
              "27            md moyeen uddin       1\n",
              "28                 abdul ahad       1\n",
              "29               mariam begum       0\n",
              "...                       ...     ...\n",
              "13565229          most. sonia       0\n",
              "13565230         dulal bepari       1\n",
              "13565231       morsheda begum       0\n",
              "13565232          most. bithi       0\n",
              "13565233   akter hossen dewan       1\n",
              "13565234           rani begum       0\n",
              "13565235         niru sultana       0\n",
              "13565236   jasim uddin sikder       1\n",
              "13565237           hena begum       0\n",
              "13565238          irina begum       0\n",
              "13565239         babul sikder       1\n",
              "13565240  shamsun nahar begum       0\n",
              "13565241          most. amena       0\n",
              "13565242   md. mosharraf akon       1\n",
              "13565243       most. hosneara       0\n",
              "13565244          afroja sumi       0\n",
              "13565245         chunnu dewan       1\n",
              "13565246         masuda begum       0\n",
              "13565247         fatema begum       0\n",
              "13565248       a. majid molla       1\n",
              "13565249         most. jarina       0\n",
              "13565250    md. rahat hossain       1\n",
              "13565251      dulal chowkider       1\n",
              "13565252         rehena begum       0\n",
              "13565253   fatema khanam urmi       0\n",
              "13565254         mohammad ali       1\n",
              "13565255   shirin akther ranu       0\n",
              "13565256       md.mizan uddin       1\n",
              "13565257          mesba uddin       1\n",
              "13565258         nasima akter       0\n",
              "\n",
              "[13565233 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "3ebTNaxQvPdk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4fc10702-ee9a-4794-da3e-03772a02f39f"
      },
      "cell_type": "code",
      "source": [
        "len(df.drop_duplicates())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3030181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "r1X7Jn3bvrGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31f9ffd0-fa21-40d9-e6fe-346fd2a02eb7"
      },
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13565233"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "eFSWrxWrrjsH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Testing"
      ]
    },
    {
      "metadata": {
        "id": "B-E6wTarfhma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LOAD SAVED MODEL\n",
        "from keras.models import load_model\n",
        "\n",
        "#del model  # deletes the existing model\n",
        "model = load_model('char_rnn_hsc_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gYoyHX_R4jrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = np.array([y[0] for y in model.predict_classes(x_test)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooq7YwUjMz4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df['en_name'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_cxMSU00oeN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3446a967-2416-42e0-f677-8722a3036ed1"
      },
      "cell_type": "code",
      "source": [
        "print(idx2char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'c', 1: '(', 2: 's', 3: ')', 4: 'a', 5: 'n', 6: 'y', 7: 'g', 8: '.', 9: 'i', 10: 'd', 11: 'v', 12: 'm', 13: ':', 14: 'f', 15: '-', 16: 'x', 17: 'q', 18: 'o', 19: 'l', 20: 'e', 21: 'k', 22: 'z', 23: 'u', 24: 't', 25: 'p', 26: 'b', 27: ' ', 28: 'w', 29: 'r', 30: 'j', 31: 'h'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pruczVOgP8Di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cc1d36aa-dd80-4e19-faf3-1f410e204e19"
      },
      "cell_type": "code",
      "source": [
        "X1 = np.array(names2tensor(df['en_name']))\n",
        "print(names2tensor(df['en_name']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0 ... 19  4 12]\n",
            " [ 0  0  0 ... 19  4 12]\n",
            " [ 0  0  0 ... 18 23  2]\n",
            " ...\n",
            " [ 0  0  0 ...  2  4  5]\n",
            " [ 0  0  0 ... 12  4  5]\n",
            " [ 0  0  0 ... 24 20 29]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jmlkMEGvINzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "b90a6f90-4d40-4d96-beb5-af7a23c432e6"
      },
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(char2idx)\n",
        "print(len(char2idx))\n",
        "print(idx2char)\n",
        "for i in range(10):\n",
        "  for x in names2tensor(df['en_name'])[i]:\n",
        "    print(idx2char[x], end=\"\")\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26940,)\n",
            "{'c': 0, '(': 1, 's': 2, ')': 3, 'a': 4, 'n': 5, 'y': 6, 'g': 7, '.': 8, 'i': 9, 'd': 10, 'v': 11, 'm': 12, ':': 13, 'f': 14, '-': 15, 'x': 16, 'q': 17, 'o': 18, 'l': 19, 'e': 20, 'k': 21, 'z': 22, 'u': 23, 't': 24, 'p': 25, 'b': 26, ' ': 27, 'w': 28, 'r': 29, 'j': 30, 'h': 31}\n",
            "32\n",
            "{0: 'c', 1: '(', 2: 's', 3: ')', 4: 'a', 5: 'n', 6: 'y', 7: 'g', 8: '.', 9: 'i', 10: 'd', 11: 'v', 12: 'm', 13: ':', 14: 'f', 15: '-', 16: 'x', 17: 'q', 18: 'o', 19: 'l', 20: 'e', 21: 'k', 22: 'z', 23: 'u', 24: 't', 25: 'p', 26: 'b', 27: ' ', 28: 'w', 29: 'r', 30: 'j', 31: 'h'}\n",
            "cccccccccccccjubiada alam\n",
            "cccccccccccccshamsul alam\n",
            "cccccccccjannatul ferdous\n",
            "ccccccccmd. shadman sakib\n",
            "ccccccccccccccccmd. mamun\n",
            "ccccccccccshahanur  begum\n",
            "cccccca.k.m. abirul haque\n",
            "cccca.k.m. ashraful haque\n",
            "cccccccccccccsamsun nahar\n",
            "ccccccccccccccmd. al-amin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H2upd8hF5dC1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#name = [idx2char[x] for x in x_train[1]]\n",
        "#print(name)\n",
        "#print(np.array(names2tensor([\"masum\"])))\n",
        "\n",
        "def nameTest(name):\n",
        "  result = model.predict_classes(np.array(names2tensor([name])))[0][0]\n",
        "  if result:\n",
        "    print(\"Male\")\n",
        "  else:\n",
        "    print(\"Female\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4kzFOAmfHFsc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21df380b-233d-4f5c-a8fa-c76c7a3bc241"
      },
      "cell_type": "code",
      "source": [
        "#model.predict_classes(nameTest(\"masum\"))\n",
        "name = \"shamim\"\n",
        "nameTest(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Female\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WU4QlpI64IuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "c35fec15-f3b5-47c1-faf1-e3d0c9146e29"
      },
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred), classes=['Female', 'Male'], normalize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[4996  122]\n",
            " [ 119 6310]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVcAAAEmCAYAAADWT9N8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmYVMXVx/FvDww7qKCyaVBDPIoa\nV1QUlFXFNYCoAYmivmpc3uCShLiAaIxJNC4RYsIrxiVqUOKuEQVFUQTRYCJRT1REkUUWBUVZZGbe\nP+rO2AwzPT2m7/Ttmd/Hpx+6697bVe0wh+pTdatSZWVliIhIbhXluwEiIvWRgquISAwUXEVEYqDg\nKiISAwVXEZEYKLiKiMSgcb4bIHXLzFLARcAZQDHh78BU4Bfuvua/eN+/AIcDZ7n71FpeeyBwjbsf\n+W3rzzUzOxn4u7t/XsWx64AP3f2Pdd8yKRQpzXNtWMzsN0BvYLC7LzazlsAtgAGHufu3+gthZiXA\nru7+fs4am0dm9g7Q390/zndbpDApuDYgZtYWWAzs6+7vpJU3AwYATwJNgJuBPkAp8BTwM3cvMbOF\nwHXAmcCOwH3ufomZzSD0Wt8H/hf4A3Cqu78Uvf9C4FRgNvBHoBfQCPgXcDqwH3C7u3eN2lKr+qv4\nnDOAp4ETgK7AVcA2URtKgWPc/QMzM2AS0I7Qi7/S3e83szuAkdHnOR04C/gU6A9cAxwDvEfo8f8N\n6Obua83ssuj/7dAsfhxSzynn2rAcDHycHlgB3H29uz/u7qXAKELg2oMQ9HoBP0w7/TCgB7A/cKGZ\n7eDuvaNjvd39qQz1HwnsDOwGfA/4d/Re6WpdfzV1HRZdOxL4bfS5dwPeIqREAG4AnnD33aOySWZW\n7O7lx3uX/wMB9AMOdPcHyytw97nAw8BlZtYZOI/wj4uIgmsD0xb4pIZzjgEmuvsmd18H3AsckXb8\nPncvcfcl0XvtWIv6VwDdgEFAC3e/sor8bK7qf9zdNwFvAi2AKVH5m0Cn6PkJwPXR85eAZkDHat5v\nuruvr6L8cmAo8GdC3nhpNddLA6Pg2rCsBDrXcM52wGdprz8Dtk97nT7oVUL4ep8Vd38VuDB6LDOz\n+8xs65jq/yLtHNx9bRXXHAm8aGb/IfRoU1T/O/FpNZ9pLfAA0JPwD4EIoODa0MwG2pvZfumFZlZs\nZteaWQtCb7Bd2uF21Nzbraxy0Num/Im7T3H3PkAXQo/yp5WuzUX9NTKzYuBB4Fp33xXYG6j1AISZ\ndQKGAfcDY3PaSCloCq4NiLuvJuQf7zazrgBRQJ1IGIj5CngCONPMGkUzCUYQBrpqYykhWJVPaWoW\nPR9pZldGbfkUeIctA1ou6s9Gy+jxWvT6J8BGoFX0ehNQuVddld8T/p+OAk42s31y3E4pUAquDYy7\nX0UIpo+ZmQOvE3qGg6NTbgUWEQabXiMEuwe3fKeMrgEuNrP5wO6Er9wAjwL7m9m7ZvY2If96Y6Vr\nc1F/jdL+oZlnZvMIMwMeAZ6IgvoDwCwzO6m69zCzYwgDdH9y9y+Ay4D/M7OsUyVSf2kqlohIDNRz\nFRGJgYKriEgMFFxFRGKg4CoiEoMGuypWp3Mfqncjec9d2Y++10zPdzNybsH4wTWfVGCaNIKNJflu\nRe41a0wql+/XfN8Lsv49XTdvfE7r/m+p51qP7NZ5q3w3QbJUlKgwIHFosD1XESkAqcLt/ym4ikhy\nFRXu/RgKriKSXKnCzZ8ouIpIciktICISA/VcRURioJ6riEgMcjigZWbDgZ8RlpMcQ9jD7R7C2sNL\ngRHuviE6bxRhv7WJ7j4pWv/3TsI6xCXASHdfkLHpOWu5iEiupVLZPzIws3aExcx7AscStvi5Gpjg\n7r0IG06eES03OYawGWVv4KJoY89hwGp37wlcS9goMyP1XEUkuXKXFugPTIvW3f0CONvMPgDOjY4/\nDlwKODDX3dcAmNnLwKGEDSrvjs6dBtxRU4XquYpIcuWo5wrsBLQws8fMbKaZ9QNauvuG6PhywuaU\nHQgbaVJdebRLcpmZNclUoXquIpJcueu5pgj7sQ0i5E2fj8rSj1d3XW3KK6jnKiLJ1ahR9o/MPgFm\nRVu2v09IDXxhZs2j452BJdGjQ9p1W5RHg1spd9+YqUIFVxFJrlRR9o/MngH6mllRNLjVipA7HRId\nHwI8DcwBupvZ1mbWipBvnRldPzQ69zhCzzcjBVcRSa4c5VzdfTEwhbC9/N+BCwmzB04zs5lAW+Au\nd18HjAamEoLvuGhwazLQyMxeAs4HflFT05VzFZHkyuFNBO7+J+BPlYoHVHHeFEIgTi8rAUbWpj4F\nVxFJLt3+KiISAy05KCISA60tICISA6UFRERioJ6riEgM1HMVEYlBUeGGqMJtuYjUf+q5iojEQDlX\nEZEYqOcqIhID9VxFRHIvVaTgKiKScymlBUREYlC4sVXBVUSSSz1XEZEYKLiKiMSgSANaIiIxKNyO\nq4KriCSX0gIiIjFQcBURiYGCq4hIDFJFCq4iIjmnnquISAwUXEVEYqDgKiIShxzFVjPrDTwI/Dsq\nehP4LXAP0AhYCoxw9w1mNhwYBZQCE919kpkVA3cCXYASYKS7L8hUZ+He/iAi9V5RUVHWjyy84O69\no8eFwNXABHfvBbwHnGFmLYExQH+gN3CRmbUFhgGr3b0ncC1wXY1t/1afWESkDqRSqawf30Jv4LHo\n+eOEgHoQMNfd17j7OuBl4FCgH/BwdO60qCwjpQVEJLFynHPtZmaPAW2BcUBLd98QHVsOdAQ6ACvS\nrtmi3N1LzazMzJq4+8bqKlNwFZHkyl1sfZcQUB8AdgGeZ/P4V11NtS2voLRAgWhWXMSsa47gpB7f\noWv7Vjx0yWH87eJeXH/qvjRKm2j91Og+PPrTwzl6304VZecO+B7PXt6Xp0b3Ye8u2+Sj+Q3Sv+fP\np5t9l9smjAdg0aJFHH1kfwb0PZz+/fuzbNkyAB58YDI9exzIYYcezNgrL89nkxMnV2kBd1/s7pPd\nvczd3weWAduYWfPolM7AkujRIe3SLcqjwa1Upl4rKLgWjJ8cvRurv/wagMsH78mtTztDbpzJ4k/X\ncfz+nWnXuikAg254gZNumsm5/b9Hs+Iidu3YmhMO2IGjrnuen987jwF7dchUjeTIl19+ycWjLqRP\nn34VZePGXMEZZ53Ns8+9wKBBg/j9zTfy1VdfccVlP+fvz0znhZde4bnp03j7rbfy2PJkydWAlpkN\nN7NLo+cdgPbAn4Eh0SlDgKeBOUB3M9vazFoRcqszgWeAodG5xxF6vpnbXvuPK3Wta/tW7NqxDdPn\nh57Oztu34o2FnwEw461POKxbe3Zs1wKADZtK2bCplPkfr2HfndsyYK+OPP76x5SUlvHmotXc8MTb\nefscDUnTpk155PGn6Njpm28Qt4z/A4MGh9/l7bbbjk9XraJFixa8Nu9NWrduTSqVol27dqz6dFW+\nmp08qVo8MnsMONzMZgKPAj8GLgdOi8raAndFg1ijgamEgatx7r4GmAw0MrOXgPOBX9RUoXKuBWDM\nid/n8r++wUk9ugDwzuLP6bdnB6bM+Yje3dqzXeumLFy+FoC2LZuwflMJ3Xdpy+z/rGCHdi0oLSvj\n3gsPpXGjFOMefJO3Fq/J58dpEBo3bkzjxpv/erVs2RKAkpISJkyYwOjLxwDQunVrAOa/+SYffriQ\ngw46uG4bm2C5GtBy9y8IPc7KBlRx7hRgSqWyEmBkbeqMLbia2U6EibqvpxW/4e6jcljHQmBPd1+b\nq/dMmhMP+g6vL1jFolVfVZRd/bc3ue6H+3BSj+8w+92VpFIpVn8VUgZ3nteDT9asx5d+HuWioIgU\nw299mQO/244bRuzH0b+u8RuNxKSkpIQzTh9B37596dP3m5TBe+++y+kjhnHn3fdRXFycxxYmi+7Q\nqp67e++Y66jX+u/Vge9s25L+e3Wk4zbN2biphKWfreO0P7wCwOHdtmf7rZpVnH/89S8A8Iczu7No\n1Zd87/PWvLfsCwBefX9VRfpA8uPsM0fStev3GDt2LOs3hbKPP/6Yk078AZP+fA9777NPfhuYMAqu\ntWBm1wK9CLecjXf3+83sTsJ8sv2B7YDfELrg2wKHA2XAfUBLoAVwobu/mvaenYBJQBPCrWlnuftH\ndfWZ4nTu7RUfk0uO3Z1Fq77koK7b0qRxI6bPX8YpPbowZc6iihkDTRsX0aZFMXvssBX//HA1ACMO\n24VHXvuYru1bseSzdXn5HAL333cvTZo04cqx4zYr//HZZ/L78bex73775allyVXISw6mysrKYnnj\nKC0wxd0PSCvrBZzj7qeaWVPgH8ABwG3AEne/zMzuJdxmdr6Z3QP8DXgL6Obuj5hZX+B8dx9SnhYA\nbgHud/dpZnY0MMjd/ydT+95ZvKZst85b5fpjiwDw+uuvc8kll7Bw4UKKi4vp3Lkzy5cvp1mzZrRp\n0waAbt26MWrUKPbZZx8OPPDAimsvvvhijj/++Hw1/VtbvwmaNc7trle7XPxU1gFqwY1HJyoSx91z\nNTObkfb6eeDgtLIiwt0PAOVdtKXAO9HzT4Ctoj+vjKZSNAW+rFTPIVFdVxB6xCuoQd9rptfqgxSC\nJX8cTKdzH8p3M3JuwfjB+W5Cre2x9/48PW1GtcebNaYiLfDp519tcbz8WEOntED1Nsu5mtlFwCR3\n32zRAzMDSP/rlP48RVihZrG7jzCzA4AbKtWzERjq7ktz2HYRybMCjq11Ps91DnCcmRWZWTMzuzXL\n67YF3o+eDyLkViu/7w8AzKyvmQ3LSWtFJK9iXrglVnUaXN19FiE18ArwIptP08rkbuBiM3uGEEg7\nmFn6nLOrgB+Y2YvA2Oj9RaTApVLZP5ImtgGtpOt07kP17oMr51o40nOu9UmuB7R2Gz0169/Td359\nZKJCrO7QEpHEKirgqVgKriKSWEn8up8tBVcRSawkDlRlS8FVRBKrgGOrgquIJFeWGw8mkoKriCSW\neq4iIjFQzlVEJAYFHFsVXEUkudRzFRGJgW4iEBGJQQF3XBVcRSS5lBYQEYlBAcdWBVcRSS71XEVE\nYqABLRGRGBRwx1XBVUSSS2kBEZEY5Dq4mllzYD5wDTAduIewY/RSYIS7bzCz4YRNUUuBie4+ycyK\ngTuBLkAJMNLdF2Sqq3CXnBGRei+GPbSuAD6Nnl8NTHD3XsB7wBlm1hIYA/QHegMXmVlbYBiw2t17\nAtcC11V+48oUXEUksYqKUlk/amJmuwHdgCejot7AY9HzxwkB9SBgrruvcfd1wMvAoUA/4OHo3GlR\nWea2Z/8xRUTqVo631v4dcHHa65buviF6vhzoCHQAVqSds0W5u5cCZWbWJFNlCq4ikli5SguY2Y+A\nV9z9g+qqylF5BQ1oiUhiFeVuQOsYYBczOxbYAdgArDWz5tHX/87AkujRIe26zsDstPJ/RoNbKXff\nmKlCBVcRSaxcxVZ3P7n8uZldBSwEDgGGAH+J/nwamAPcbmZbA5sIudVRQBtgKDAVOA54vqY6lRYQ\nkcRqVJTK+vEtjAVOM7OZQFvgrqgXO5oQRKcB49x9DTAZaGRmLwHnA7+o6c2r7bma2RmZLnT3O7L+\nCCIi30IcNxG4+1VpLwdUcXwKMKVSWQkwsjb1ZEoL9MpwrAxQcBWRWBXwDVrVB1d3r4jSZlYEbO/u\ny+qkVSIiQKrmQfnEqjHnamZ9gfeBGdHrm8zsmJjbJSJCUSr7R9JkM6D1K+Bgwr23EG79ujK2FomI\nRHJ5h1Zdyya4rnX3T8pfuPtKIOP8LhGRXChKpbJ+JE0281zXmdnhQMrMtgFOAdbH2ywRkXo6oJXm\nPOA2oDsh9zoTODvORomIQD1fz9XdFwHH1kFbREQ2U8CxtebgamaHEVaT6UZYPHY+cKm7vxxz20Sk\ngWtUwNE1m7TAeMK9tbMIK8H0BP4A7B1ju0RE6ndaAFju7s+lvX7WzD6Kq0EiIuUSOMMqa5nWFtgl\nejrXzC4BniWkBfoB/6iDtolIA1dfe67TCWsIlH+6C9KOlRFWlBERiU0Bx9aMawvsXN0xMzsknuaI\niHzjWy4lmAjZzBZoA5wKbBsVNSUsvdUpxnaJiBR0WiCb218nA98nBNTWhDmvP46zUSIiEHKS2T6S\nJpvg2szdzwU+dPefAn2Ak+JtlohIYa8tkE1wbWpmLYEiM2vn7p8C3425XSIiOdv9NR+ymed6N/A/\nwO3A22a2Angv1laJiEAilxLMVjZrC/yx/LmZTSfsSDAv1laJiJDTrbXrXKabCK7OcGyQu4+Jp0ki\nIkEBx9aMPdeSOmtFHiwYPzjfTYhFffxc23S/oOaTCsy6eePr7efKpUKeipXpJoJxddkQEZHKshlx\nT6psBrRERPKiXt+hJSKSLwUcW7MLrmbWDtjZ3V8zsyJ3L425XSIiOcu5mlkL4E6gPdAMuAb4J3AP\n0Iiwu/UId99gZsMJa1iXAhPdfZKZFUfXdyGMR4109wWZ6qwxpWFmPwRmR28McKuZnVnbDyciUltF\nqewfNTgOeM3dDyfcYXojcDUwwd17EebunxHdMDUG6A/0Bi4ys7bAMGC1u/cErgWuq6nCbHquFxN2\nHXgyen0pMAOYlMW1IiLfWq4mC7j75LSXOwIfE4LnuVHZ44TY5sBcd18DYGYvA4cS1rG+Ozp3GnBH\nTXVmMxi3xt2/SmvkOmBjFteJiPxXGqdSWT+yYWazgPsIX/tbuvuG6NByoCPQAViRdskW5VFatMzM\nmmRsexbtWWlmpwHNzWw/4ORKlYuIxCLX01zd/RAz2wf4C5svplVdTbUtr5BNz/VcoDthucHbgebA\nWVlcJyLyX8nVqlhmtr+Z7Qjg7m8QOpZfmFnz6JTOwJLo0SHt0i3Ko8GtlLtn/AafzdoCq9l8ixcR\nkTqRw57rYYSR/lFm1h5oBTwNDCH0YodEr+cAt5vZ1sAmQr51FNAGGApMJQyOPV9ThdnsRLCIsGfW\nZtz9O1l9JBGRbymH81z/CEwys5mEb9/nA68Bd5vZOcCHwF3u/rWZjSYE0TJgnLuvMbPJwAAzewnY\nAJxeU4XZ5Fx7pj1vQhg1a17NuSIiOZOrO7SigfhhVRwaUMW5U4AplcpKCLuxZC2btMCHlYreNbOp\nwE21qUhEpLbq9R1aZta3UtGOaCcCEakDqUTujpWdbNICV6Y9LwM+55uJtyIisanXPVfgEnf/R+wt\nERGppJCDazbzXG+IvRUiIlVoVJTK+pE02fRcPzKzGYTFWyomzWqbFxGJWwFvRJBVcP0geoiI1Kn6\nukHhcHe/V9u9iEi+JPDbftYy5Vy1ZquI5FUqlf0jabTNi4gkVqMkRs0sZQquh5jZR1WUp4AyrS0g\nInEr5LRApuA6DzilrhoiIlJZvRzQAtZXsa6AiEidKeDYmjG4vlpnrRARqUK97Lm6+8/rsiEiIpU1\nKtzYqtkCIpJcqfrYcxURybfCDa0KriKSYPUy5yoikm+FG1oVXEUkwYoK+C4CBVcRSaxsFpxOKgVX\nEUkszRYQEYlB4YZWBVcRSTD1XEVEYlBflxwUEcmrwg2tCq4ikmC57Lia2W+BXoS4dx0wF7gHaAQs\nBUa4+wYzGw6MAkqBie4+ycyKgTuBLkAJMNLdF2Sqr5BnOohIPVdEKutHJmbWB9jT3XsARwE3A1cD\nE9y9F/AecIaZtQTGAP2B3sBFZtYWGAasdveewLWE4FxD20VEEiqHe2i9CAyNnq8GWhKC52NR2eOE\ngHoQMNfd17j7OuBl4FCgH/BwdO60qCwjBVcRSaxULf7LxN1L3P3L6OWZwFNAS3ffEJUtBzoCHYAV\naZduUe7upUCZmTXJVKdyriKSWLmeLWBmJxCC6xHAu2mHqquotuUV1HMVkcTK5dbaZnYkcDkw0N3X\nAGvNrHl0uDOwJHp0SLtsi/JocCvl7hsz1afgKiKJlavgamZbAdcDx7r7p1HxNGBI9HwI8DQwB+hu\nZlubWStCbnUm8Azf5GyPA56vqe0KrgXm3/Pn082+y20TxleUTbj197RuXszatWsrym6f+CcOPbg7\nfQ47lIcf+ls+mtognTLwAOZMHs3L9/6Mo3ruwUHf35npd1zE0xP/l0fHn8e227SqOPfR8edx3/Vn\nVrxu3LiIP197GtPvuIhnbv8JO3Vul4+PkCi5yrkCJwPbAg+Y2Qwzm0EY9T/NzGYCbYG7okGs0cBU\nQvAdF/VyJwONzOwl4HzgFzVVqJxrAfnyyy+5eNSF9OnTr6Ls3nvuZvnyT+jYqVNF2fLly7n5pht4\nbd6bABw1oC9HDTya5s2bb/Gekjttt2rJZecczSHDfkOrFk254txj2KZNc8688m4WLl7FZWcPZOSg\nQ7j+jmcAmPXG++xtO1Rcf/LA7qz+Yh0jL7+JfgfvxjUXHs+I0X/O18dJhFytOOjuE4GJVRwaUMW5\nU4AplcpKgJG1qVM91wLStGlTHnn8qc0C6fE/GMS4a67d7B7sDxcuxGw3mjVrRrNmzdh7732Y++qc\nfDS5Qel7kPHcnHdY+9UGlq38nAt+eT/Df3YHCxevAqDT9luzePnqivNnzdt8DnqfA3flsef/CcBz\nc5we++xSd41PqKJUKutH0ii4FpDGjRtv0fts3br1Fud9t2tX5s9/k5UrV7J27VpmvzKLTz75pK6a\n2WB16dSWFs2a8ODN5zBt0ih6H7grAAMO2Z1/PTKG7du15v4n51Z7fft2bVj5WUjtlJWVUVYGxY0b\n1UnbkyqHaYE6l5jgamY7mVmZmR1cqXyumd1ZzTWnm9kNddLAAtK2bVt+9evrOXHQ8Zx1xmns3m0P\nysrK8t2sei+VStF2q5accsn/8T9j/8LEq04F4NlZb/P9H1zNfz74hEtHbvEtNMP7xdXSwlGUyv6R\nNIkJrpEFwA/LX5hZV2Cb/DWncA05cSgzZs7irw/8jdKyUrp02SnfTar3Pln1BbP/9QElJaV88PFK\nvvhqAycesV/F8Uemv8Eh+3632uuXrlhD+3ZtgDC4lUql+HpTSeztTjL1XHNnNjDAzMq/C51CmAKB\nmQ03s9lm9rKZbZGYNrPzo2MzzeySOmxz4mzatIkj+vVm/fr1LFu2jH/98w32P+CAfDer3pv+ytv0\n7r5rRQ+2VYum/Pyso/j+rp0B6L7XTry7sPr0zPRX3mHwgH0BOOawvXhh7n/qpN1Jlst5rnUtabMF\nvibMM+tDmAZxAjAOOJFwL/BR7r7azF40s73KLzKznaNzekZFL5vZg+7+UZ22Pmb/eP11Rv/sEj78\ncCHFxcU8/NAU+vUfwPRpz/LJsmUMHDiQ7gf14Fe//i2Dhwyld88epFIpbrplPI0bJ+1HXf8sWbGG\nh6fN48W7w7/tF//mQZauWMMtl53MppJS1q3/mjOvuKti073rfzqETttvxdT/+wm/mvh3Hnzmdfoe\nvBvT77iIDRs3cfbYe/L5cRKhkNdzTSUlF2dmOwFXAQ8QJvTeBPySsHrN6cATwIXR6d0IE3p3AvYk\nLB32O8LKNgDtgPPd/cXq6istoyyJeRqRQtV83wtYN298Tn+rZr+3OusAdXDXrRP1G53E7sw0YDxh\nfcXyuWZNgAnA3u6+zMyeqHTNRuBJdz8n20o21sNUVrPGsH5TvluRe9t0vyDfTci5dfPG03zf+ve5\nci5R4bJ2kpZzJbpf90XC4gqPR8WtgU1RYN0ROIAQcMu9DvQxsxZmljKzW9LuGRaRAqUBrdx7EPhH\ndNsZwCrgWTObC4wFfktIGxQDRLnVmwlBeTawLLqNTUQKWCEPaCUm51rX1m+i3n1wpQUKR31NC+Q6\n5zr3gzVZ/55233mrRIXYJOZcRUQAEvl1P1sKriKSWEn8up8tBVcRSawCjq0KriKSYAUcXRVcRSSx\nkriUYLYUXEUksQo3tCq4ikiSFXB0VXAVkcTSVCwRkRgUcMpVwVVEkkvBVUQkBkoLiIjEQD1XEZEY\nFHBsVXAVkQQr4Oiq4CoiiaU7tEREYpDL0GpmewKPAje5+/hoV5N7gEaEbaVGuPsGMxsOjAJKgYnu\nPsnMioE7gS5ACTDS3Rdkqi+pOxGIiITomu0jAzNrCdwKTE8rvhqY4O69CJubnhGdNwboD/QGLjKz\ntsAwYLW79wSuBa6rqekKriKSWDncQ2sDcDSwJK2sN/BY9PxxQkA9CJjr7muiraJeBg4F+gEPR+dO\ni8oyUnAVkcTK1R5a7r6pin31Wrr7huj5cqAj0AFYkXbOFuXuXgqUmVn6JqlbUM5VRBKrDsezqqup\ntuUV1HMVkcSKeWvttWbWPHremZAyWELopVJdeTS4lXL3jZneXMFVRBIr5q21pwFDoudDgKeBOUB3\nM9vazFoRcqszgWeAodG5xwHP1/TmSguISGLlKitgZvsDvwN2Ar42sxOB4cCdZnYO8CFwl7t/bWaj\ngalAGTDO3deY2WRggJm9RBgcO72mOhVcRSS5chRd3f11wuyAygZUce4UYEqlshJgZG3qVHAVkcTS\nHVoiIjEo3NCq4CoiCVbAHVcFVxFJssKNrgquIpJY6rmKiMSgSMFVRCT3tIeWiEgcCje2KriKSHIV\ncGxVcBWR5NKAlohIDFIFHF0VXEUksQo3tCq4ikiCFXDHVcFVRJJLU7FERGKgnquISAwUXEVEYqC0\ngIhIDNRzFRGJQQHHVgVXEUmwAo6uCq4ikljaQ0tEJAaFG1oVXEUkyQo4uiq4ikhiFfJUrFRZWVm+\n2yAiUu8U5bsBIiL1kYKriEgMFFxFRGKg4CoiEgMFVxGRGCi4iojEQMFVRCQGCq4iIjFQcBURiYGC\naz1nZoV7/2A9p59N/aa1BeoxM0u5e1n0/EeEn3fK3Sflt2VS6WfTD9gemAasLC+Xwqaeaz2W9st7\nNjAEWAyca2ZD89owSf/ZXAhcDOwLPAXsnc92Se4ouNZDZnaYmY2InhcDBwLnA12Bt4EpZrZ7HpvY\nYJnZbmY2Mnq+FXCgux8DvAksdPc3ytMFShsUNgXX+qkU+J2ZneruXwNrgBuA7sCZhPTAsfrlrVtm\n1ojQQ+1hZsPdfQ2w0cyeAwYCJ5nZPsA4+KZ3K4VJwbUeMbOUmRW5+0vAD4BxZnYCMBnoAdwfBduT\ngGOAVvlrbcMS5VhLgKeBZ4Hs2D2hAAAG/0lEQVRDzWwY4WezEXgyCqZdgfZm1jx/rZVc0Hqu9USl\nAZJO7r7EzPYEHgXOAzYBPwVWAF2As939nbw1uAGJ/sErjZ63Br4CTiR8k1gOvA+cC3wO7AIMd/e3\n8tRcyREF13rGzM4DTgBWAn8EPiME2POBF4G2QKm7L8lbIxsoMzsLGAzMBJ4A9iQMYL1N+BntBnyk\nn039oKlY9YiZlX/d/yFwPXAL8DOgN2HA5Ax3fyhvDWxgzOwQwoDVzWZ2LDACOBV4DtgW+CvhG0U/\nYL27T85bYyXnFFwLWPrXzcgK4C/ASEI+dSzwS+Bu4BrgX3XeyAYoGihsBIwB9o4Gsj4DLiIMXP2b\nMK/1NOBjYA4h4Eo9orRAPRD1it4D1hH+wfwVcIW7v2tm44F9gFPc/eM8NrPBiUb+ryME0GLgQWCI\nu59hZm2Ax4GPgGvc/T/5a6nEQcG1AFUavDoeuBX4B2EU+lng7Oh16+hxm7t/lafmNlhmtj1wBTCX\nkAboBhxNSN20I/Rir3L3tXlrpMRGwbWAmVkf4HvAM0BL4BzCV/8dCT3YgcCP3H1+3hrZgJjZkYTB\nxAfcfUZUNgI4i5CW2R24EPiacLfcT9z97fy0VuKmnGuBin5pxwFLCfnVW4AHCL/ciwjzKa9Tr6hO\ntQSGAf3M7EVgAuGW1mLCLI0XgZ2A5oQe6/I8tVPqgG4iKEBm1hs4AOgJ/AbYmZDLewl4EtgBWKHA\nWreimRjHEFIzOwF9gLuiP3d3938Ck4AxCqz1n4JrAUi/TTUaeT6BMCdyF3d/DHgF6GVmP4q+jo51\n98/y0lh5hTA74wPgQ8KAVhfgUjM73d3fcveV+Wyg1A0F1wJiZvsBRrjTajZwvJl1c/f7gHnA982s\ntbuvy2c7G7JoatxcwpS44wlTsPoT7sh6Jo9NkzqmAa0Ei1auWuTua6Ol6QYSplutBy4HLgDKgLvd\n/U0za+Pun+evxZLOzA4nDGBd7+5z8t0eqVvquSaUmfUFfkdYxKMb0NvdjyZMNm/u7guB0UAT4GQz\na6LAmizu/gJwE2EuqzQw6rkmUDRgNRa43N1nRfMlfw2kgGaE2yj7E+awPgG01gCJSLKo55ow0ZYf\ndwNnuvssgChwvk0YGLnB3TcBHQnzJtcrsIokj+a5Jk9ToIQwvWoBgJn9kjCHcgYwxsw+Bg4BhmlB\nZZFkUloggaI7fa4GriLMDjjQ3YeZWSvCMnV7Ac9GeVcRSSAF14Qys4GEO3xWu/t+UVlxtJOAiCSc\ncq4J5e5/JyxwXWJmR0RlCqwiBUI914SLUgTjgUuiu7FEpACo55pw7j4V+DFhJwERKRDquYqIxEA9\nVxGRGCi4iojEQMFVRCQGCq4iIjFQcBURiYHWFpDNmNlOgBNW1Iew/9OHwHnuvvpbvudZQE93P93M\n/kqYs7u4mnMPAZa5+4Is37sx8LW7pyqVXwU0dvcrMly7EOjv7u9lWdedwEvufns250vDpuAqVVnh\n7r3LX5jZ9YQtoi/9b9/Y3U+p4ZSRwGSiRWtECpWCq2TjRcK23eW9vcmE/buGmtlJhNX2U8AK4Cx3\nX2Vm5wHnEXaiXVL+RuW9RULw/D1ho0UIC4NvAoYCB5rZRcB7wB+AFoQdbi9z92lmZoRtVL4Cnq+p\n8Wb2Y+BHwEbCLg4np/XCzzKz7kB74AJ3n2Fm36mq3lr8/xJRzlUyizZEHAzMTCt+NwqsOxK2m+nv\n7j0JSyJeZmZbAdcAh7v7QGDbKt56ONDe3Q8GjgJOBx4D3iCkDZ4DbgN+5+59CftR3R6lAcYCd7j7\n4cC/svgYzYEjovMXAqemHVvl7v2AnwA3RGXV1SuSNf2FkapsZ2YzoudFhMB6U9rxWdGfPQiLdk8N\nnUmaEnY97QosdPdV0XnPA/tUquMgQjAm6kUeAxC9T7k+QGszGxu9/hrYnrDk4nVR2XNZfJ5VwFNm\nVkrY8npp2rFn0z7THjXUK5I1BVepymY51ypsjP7cALzq7semHzSzA4DStKJGVbxHGTV/c9oADK68\nFXW01Xj5+1f13unn7kDoke7h7svN7IZKp5S/T/p7VldvDc0V+YbSAvLfmEvIj3YAMLOhZnYC8D6w\ni5ltHQXCflVcO4uQDsDM2pjZHDNrQghwxdE5LwEnRedsa2Y3R+VvEXrNEPK3mWwPrIwCa1vgCEIP\nu1x52w4F5tdQr0jWFFzlW3P3JYRc5RNm9iJwJjDb3T8DriWkEx4l5DkrewD4wMxmEb6a3+juG6Pn\nfzKzwcD/AoPMbCbwFN+kAK4GzjOzqYSdGjZlaOYbwLtm9iph8fGxwEgz6xkdb2tmTwA38s1siOrq\nFcmaVsUSEYmBeq4iIjFQcBURiYGCq4hIDBRcRURioOAqIhIDBVcRkRgouIqIxOD/AU9jpxtOUaVW\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f364239b048>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5E_-Bil1DqSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "961f044d-6f26-4ec7-818b-5b209bc29605"
      },
      "cell_type": "code",
      "source": [
        "y_train[y_train == 1.0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10703,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "PY414f_cfY2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Named Entity Detection"
      ]
    },
    {
      "metadata": {
        "id": "0eHIGSMrgAqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Polygot\n",
        "http://polyglot.readthedocs.io/en/latest/NamedEntityRecognition.html\n",
        "Failed to make it work."
      ]
    },
    {
      "metadata": {
        "id": "lQZQ_MpcUT5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "81e60d3e-4ffc-405f-ca49-149e673e8185"
      },
      "cell_type": "code",
      "source": [
        "!pip install polyglot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting polyglot\n",
            "  Downloading polyglot-16.7.4.tar.gz (126kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 4.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: polyglot\n",
            "  Running setup.py bdist_wheel for polyglot ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b4/b7/34/e6fbb82ec71c0c9d7f1b26a038f00129acd99a6aa5e5b93f2d\n",
            "Successfully built polyglot\n",
            "Installing collected packages: polyglot\n",
            "Successfully installed polyglot-16.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UH3tL3d6UesI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "f0eadabd-2ddd-4644-8c31-b08e5db0102d"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install python-numpy libicu-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libicu-dev is already the newest version (57.1-6ubuntu0.2).\n",
            "libicu-dev set to manually installed.\n",
            "Suggested packages:\n",
            "  python-nose python-numpy-dbg python-numpy-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-numpy\n",
            "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 1,885 kB of archives.\n",
            "After this operation, 10.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/main amd64 python-numpy amd64 1:1.12.1-3.1ubuntu4 [1,885 kB]\n",
            "Fetched 1,885 kB in 1s (1,428 kB/s)\n",
            "Selecting previously unselected package python-numpy.\n",
            "(Reading database ... 16712 files and directories currently installed.)\n",
            "Preparing to unpack .../python-numpy_1%3a1.12.1-3.1ubuntu4_amd64.deb ...\n",
            "Unpacking python-numpy (1:1.12.1-3.1ubuntu4) ...\n",
            "Setting up python-numpy (1:1.12.1-3.1ubuntu4) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w-HQOcGnVDa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "2d3f3ced-0846-4f1d-96a8-8f5ac3e5c4f9"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install libicu-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libicu-dev is already the newest version (57.1-6ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KzbPIg3pVI8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "outputId": "168c57fd-6fcf-4957-85c5-2d90679f5ca1"
      },
      "cell_type": "code",
      "source": [
        "from polyglot.text import Text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-365fdab6978f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_morfessor_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountedVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlobComparableMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringlikeMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/load.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountedVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCaseExpander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDigitExpander\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/mapping/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountedVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVocabularyBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexpansion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaseExpander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDigitExpander\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = ['CountedVocabulary',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/mapping/embeddings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountedVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_decode'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vO4Sc61DUvuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "ea761a85-a419-478a-a79c-1c5c07fcc9e8"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/aboSamoor/polyglot.git@master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/aboSamoor/polyglot.git@master\r\n",
            "  Cloning https://github.com/aboSamoor/polyglot.git (to master) to /tmp/pip-s5fe_s9q-build\n",
            "Collecting PyICU>=1.8 (from polyglot==16.7.4)\n",
            "  Downloading PyICU-2.0.3.tar.gz (201kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 3.4MB/s \n",
            "\u001b[?25hCollecting futures>=2.1.6 (from polyglot==16.7.4)\n",
            "  Downloading futures-3.1.1.tar.gz\n",
            "Collecting morfessor>=2.0.2a1 (from polyglot==16.7.4)\n",
            "  Downloading Morfessor-2.0.4.tar.gz\n",
            "Requirement already up-to-date: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4)\n",
            "Collecting pycld2>=0.3 (from polyglot==16.7.4)\n",
            "  Downloading pycld2-0.31.tar.gz (14.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 14.3MB 94kB/s \n",
            "\u001b[?25hRequirement already up-to-date: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from polyglot==16.7.4)\n",
            "Collecting wheel>=0.23.0 (from polyglot==16.7.4)\n",
            "  Downloading wheel-0.31.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 10.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyICU, futures, morfessor, pycld2\n",
            "  Running setup.py bdist_wheel for PyICU ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/74/ee/ee/d464eefe0e8cc56f170ea09097cfbc11fc3bdf19de42d93b17\n",
            "  Running setup.py bdist_wheel for futures ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ad/79/48/b32521764d59b16fd1bc0ffd5862f6d3bf770c7d73ea1fb12a\n",
            "  Running setup.py bdist_wheel for morfessor ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/20/48/ec/015331b44a51ffe4cee5845b24469218ffc547a504edb14fe5\n",
            "  Running setup.py bdist_wheel for pycld2 ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f0/ed/93/4e70bc7622711cb867d8df5bb90ad3bc1bd5cc1659f2dc6c41\n",
            "Successfully built PyICU futures morfessor pycld2\n",
            "Installing collected packages: PyICU, futures, morfessor, pycld2, wheel, polyglot\n",
            "  Found existing installation: futures 3.0.5\n",
            "    Uninstalling futures-3.0.5:\n",
            "      Successfully uninstalled futures-3.0.5\n",
            "  Found existing installation: wheel 0.30.0\n",
            "    Uninstalling wheel-0.30.0:\n",
            "      Successfully uninstalled wheel-0.30.0\n",
            "  Found existing installation: polyglot 16.7.4\n",
            "    Uninstalling polyglot-16.7.4:\n",
            "      Successfully uninstalled polyglot-16.7.4\n",
            "  Running setup.py install for polyglot ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25hSuccessfully installed PyICU-2.0.3 futures-3.1.1 morfessor-2.0.4 polyglot-16.7.4 pycld2-0.31 wheel-0.31.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jjscOY1oVn1A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from polyglot.detect import Detector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U6YK2hBqV_Q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "9cb502b9-9461-45fb-a030-126c83d74c8e"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install libicu-dev"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libicu-dev is already the newest version (57.1-6ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "csDh6o8UWJyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDYEgvrugE_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ]
    },
    {
      "metadata": {
        "id": "Kqzg-wHIq4-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4886
        },
        "outputId": "6b260556-11a5-427f-ff80-18edcf994763"
      },
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\r\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\r\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lJEN2fikoOw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "febcab0f-f29e-4175-860c-04b50203dd71"
      },
      "cell_type": "code",
      "source": [
        "# PARTS OF SPEECH\n",
        "from nltk import pos_tag, word_tokenize\n",
        "pos_tag(word_tokenize(\"John and Smith are going to NY and Germany\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Smith', 'NNP'),\n",
              " ('are', 'VBP'),\n",
              " ('going', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('NY', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Germany', 'NNP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "ETyU9GiPpVVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dcbeb3a2-1b8d-4365-f101-9a5b5b8a4450"
      },
      "cell_type": "code",
      "source": [
        "# NAMED ENTITY TAGGER\n",
        "import nltk\n",
        "sentence = \"john is my best friend\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S john/NN is/VBZ my/PRP$ best/JJS friend/NN)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3U9FshyKsIuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8d3f51a7-092a-4ef6-98ec-b9d132e955d0"
      },
      "cell_type": "code",
      "source": [
        "tree1 = nltk.Tree('NP', ['Alice'])\n",
        "print(tree1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(NP Alice)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lznlTotGxKCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "eb09040a-5afe-42f3-e1f4-ede171fba450"
      },
      "cell_type": "code",
      "source": [
        "# NER output as a list\n",
        "\n",
        "import nltk\n",
        "\n",
        "my_sent = \"Masum and Masum is my friend\"\n",
        "\n",
        "parse_tree = nltk.ne_chunk(nltk.tag.pos_tag(my_sent.split()), binary=True)  # POS tagging before chunking!\n",
        "\n",
        "named_entities = []\n",
        "\n",
        "for t in parse_tree.subtrees():\n",
        "    if t.label() == 'NE':\n",
        "        #named_entities.append(t)\n",
        "        named_entities.append(list(t))  # if you want to save a list of tagged words instead of a tree\n",
        "\n",
        "print(named_entities)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('Masum', 'NNP')], [('Masum', 'NNP')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TIHZ9Yz31l5Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "named_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90rsZJkU4ErE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stanford NER"
      ]
    },
    {
      "metadata": {
        "id": "OJd8AJhu4UPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "7e0a353a-473f-4cce-ef65-754f604a2fcb"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   '/usr/share/stanford-ner/stanford-ner.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')\n",
        "\n",
        "text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
        "\n",
        "tokenized_text = word_tokenize(text)\n",
        "classified_text = st.tag(tokenized_text)\n",
        "\n",
        "print(classified_text)\n",
        "\n",
        "# NEED JAVA TO RUN THIS.\n",
        "# PREFER TO RUN IT ON MY PC"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3b2abb696bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n\u001b[1;32m      5\u001b[0m                                            \u001b[0;34m'/usr/share/stanford-ner/stanford-ner.jar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \t\t\t\t\t   encoding='utf-8')\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    717\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    718\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 719\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 635\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /usr/share/stanford-ner/stanford-ner.jar"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TiO6_fYF70u0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}